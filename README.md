# 유사도 기반 법률 질의응답
## 1. 프로젝트 개요
* 법률과 관련된 질문을 입력받고, 해당 질문과 가장 유사하다고 판단되는 판례 다섯 개의 요약본 출력
* 형태소 분석기를 이용해 문장을 토큰화하고 임베딩한 뒤 유사도를 계산하여여 상위 다섯 개 선정
## 2. 데이터
* 크게 두 가지의 데이터 필요. 법률과 관련된 질문과 그에 대한 답변이 있는 데이터, 그리고 임베딩을 하기 위해 모델을 학습시킬 데이터.
* 법률과 관련된 질문과 그 응답은 Ai 허브의 '법률-규정 텍스트 분석 데이터_고도화_상황에 따른 판례 데이터' 사용했다. 
  * 입력된 질문과 유사도를 검사하기 위한 질문(question)과 질문에 대한 응답으로 보여줄 판결요지의 요약본(summ_pass)을 추출하여 저장하였다.
  * 챗봇을 완성시킨 후 법률구조공단의 질의응답 데이터를 발견하여 마지막에 추가해주었다.
* 임베딩 모델을 학습하기 위한 데이터로는 ai 허브의 대규모 구매도서 기반 한국어 말뭉치 데이터의 법률, 법학(TL360) 폴더의 데이터를 사용했다.
## 3. 형태소 분석기와 임베딩 모델 선택
* 이 프로젝트를 진행하기 위해서 몇 가지 정해야 할 것이 있었다. 어떤 형태소 분석기를 선택하고 임베딩은 어떤 것을 이용할 것인지 등.
  * 각 모델과 기법들을 비교하기 위하여 또다른 질문과 답변 쌍이 필요했다. 유사도 기반이기 때문에 데이터를 나누어 검증 데이터를 이용해서 정답을 확인할 수 없었다.
  * 그래서 생각한 것이 데이터 증강 기법 중 역번역이었다. 질문을 역번역하여 새로운 질문과 응답쌍을 만들었고, 이 중 만 개의 샘플을 뽑아 비교하는 데 사용했다.
  * 결과로 출력된 다섯 개의 요약본 중 정답이 있는 것의 비율, 첫 번째 출력(가장 유사한 요약본)이 정답인 것의 비율율 비교했다.
* 형태소 분석기는 kiwi, okt, hannanum, komoran, kkma를 비교했다. w2v를 이용하여 임베딩하고 코사인 유사도를 이용하여 유사도를 측정하였다.
<pre><code>* 두가지 다 가장 높게 나온 kkma 선택
kiwi: 0.6016, 0.4014
okt: 0.4569, 0.3026
hannm: 0.3047, 0.1823
komoran: 0.4299, 0.2926
<b>kkma: 0.6211, 0.4185</b>
</code></pre>
* 다음으로는 불용어를 정해주었다. 앞서 형태소 분석기를 비교할 때 임의로 몇 개의 불용어를 정해 진행하였는데, 이것과 지정하지 않은 것 그리고 가장 많이 나온 상위 50개의 단어를 불용어로 하는 등 세 가지를 비교하였다.
  * 상위 50개 단어는 다음과 같이 대부분 조사나 어미였는데, 계약이라는 단어가 있어서 이는 제외하고 나머지 전체를 불용어로 지정하였다.
<pre><code>['하', '의', '에', '이', '는', '을', '가', '되', '를', '있', '여', '으로', '계약', '경우', '는가', '수', '아', '로', '그', '은', '고', '제', '것', '지', '에서', '저', '대하', '조', '어', '등', '행위', '관하', '해당', '보', '항', '에게', '및', '과', '받', '와', '없', '의하', '다고', '처분', '않', '또는', '였', '었', '나요', '따르']
</code></pre>
<pre><code>* 가장 높은 최빈값을 불용어로 지정
<b>최빈값: 0.6770, 0.4636</b>
불용어X: 0.6452, 0.4491
기존: 0.6211, 0.4185
</code></pre>
* 이제 임베딩 모델을 결정할 차례이다. sbert와 fasttext, word2vec를 비교했다. sbert는 이미 학습되어 있는 모델이었고, fasttext와 word2vec는 앞서 설명한 책 데이터를 이용해 학습한 뒤 사용하였다.
<pre><code>* sbert 선택
<b>sbert: 0.7372, 0.5691</b>
w2v: 0.6770, 0.4636
fst: 0.6119, 0.4041
</code></pre>
* 마지막으로 유사도 기법 비교하였다. 코사인 유사도, 피어슨 유사도, 유클리드 유사도, 자카드 유사도를 사용하였다.
  * 자카드 유사도는 임베딩을 이용한 것이 아니라 토큰화만 되어 있으면 되었기 때문에, 각 형태소 분석기 별로 자카드 유사도를 이용한 결과를 비교했고, 역시 높은 성능을 보였던 kkma로 토큰화한 것을 최종 선택했다.
  * 만 개로 했을 때 코사인과 피어슨 유사도 결과가 너무 유사하게 나와서 샘플을 25000개로 늘리고 다시 비교했다.
<pre><code>sbert_코사인: 0.7372, 0.5691
(25000개:  0.74036)
sbert_피어슨: 0.7373, 0.5690
(25000개: 0.7344)
kkm_자카드: 0.8525, 0.6665
sbert_유클리드: 0.7286, 0.5642
</code></pre>
  * 자카드가 가장 높게 나왔으나, 자카드가 다른 유사도 기법과 달리 단어의 빈도수를 이용하는 방식이어서 두 가지를 섞어서 사용하는 걸로 결정하였다. (자카드 1, 코사인 9)
## 4. 파일 설명
* data
  * pan_qna.py: 판례 데이터 중에서 질의응답만 저장하는 코드
  * book_law.py: 도서 데이터 중에서 법률 부분만 저장하는 코드
  * scraping.ipynb: 법률구조공단 사이트에서 상담(질의응답)을 스크래핑하여 토큰화 및 임베딩을 마치고 저장하는 코드
* baseline
  * tokenize_to_kkm.ipynb: konlpy의 kkma로 토큰화(형태소 분석)하는 코드
  * embedding_to_w2v.ipymb: kkma로 토큰화한 것을 w2v로 임베딩하는 코드
  * similarity_based_chatbot.ipynb: 유사도 기반 질의응답 챗봇의 초안 코드(코사인 유사도 사용)
* validation
  * back_translation.ipynb: 질문을 역번역하고 샘플 만 개를 뽑아 저장하는 코드
  * tokenizing_n_embedding.ipynb: 각 형태소 분석기 별로 토큰화하여 w2v로 임베딩하는 코드
  * setting_stopwords.ipynb: 질문 데이터를 기준으로 가장 많이 나온 상위 50개의 단어를 확인하고, 그 단어들을 불용어로 적용한 것과 불용어를 아예 정의하지 않은 것을 저장하는 코드
  * fasttext_n_sbert.ipynb: fasttext와 sbert를 이용하여 임베딩하는 코드
  * check_chatbot_result.ipynb: 역번역 질문을 이용하여 정답을 맞힌 개수를 확인하는 코드. 각 단계 별로 후보들을 비교하였다.
* chatbot
  * similarity_based_chatbot_vs1.ipynb: 최종적으로 선택된 것들을 이용하여 질의응답을 완성하고 결과를 확인하는 코드
  * similarity_based_chatbot_vs1_add_data.ipynb: 법률구조공단의 데이터를 포함해서 결과를 확인하는 코드
## 5. 웹 (25.03.17 추가)
* 위 파이썬 코드를 이용하여 간단한 웹 페이지를 만들었다.
* flask와 gunicorn, react를 사용했다.
* 파이썬 파일: modules.py는 파이썬 파일을 정리하여 모듈을 모아놓은 것이고, forms.py는 프론트엔드(react)와 데이터를 주고 받는 파일이다.
* 리액트 파일: pan_recommand.js는 메인이 되는 페이지로, 사용자 입력을 받아 서버에 보내고 추천 결과를 받아 보여준다. loading_modal.js는 로딩 중일 때 보여줄 페이지이다.
